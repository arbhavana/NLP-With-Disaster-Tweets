{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbhavana/NLP-With-Disaster-Tweets/blob/main/Code%20file/NLP_with_disaster_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the neccessary libraries and downloading the required packages."
      ],
      "metadata": {
        "id": "fAASy6dijz6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DUKWYYZQp1E",
        "outputId": "003d859a-fd4d-4ea6-d8cd-70a4b2179936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the **Training Dataset** into **data** and **Testing dataset** into **test_data**"
      ],
      "metadata": {
        "id": "lY65UQ-tkHsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "train_file = 'final_combined_file.csv'\n",
        "test_file = 'test.csv'\n",
        "data = pd.read_csv(train_file)\n",
        "test_data = pd.read_csv(test_file)"
      ],
      "metadata": {
        "id": "vspe88uUh-uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L1iyLVOlh_lg",
        "outputId": "fc6a2fd9-3471-4ba3-8e18-98210cd3aa9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    id keyword        location  \\\n",
              "0  0.0  ablaze             NaN   \n",
              "1  1.0  ablaze             NaN   \n",
              "2  2.0  ablaze   New York City   \n",
              "3  3.0  ablaze  Morgantown, WV   \n",
              "4  4.0  ablaze             NaN   \n",
              "\n",
              "                                                text  target  \n",
              "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
              "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
              "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
              "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
              "4  \"Lord Jesus, your love brings freedom and pard...       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5aedd60-a4a5-453e-9fe8-63a7e2163593\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>New York City</td>\n",
              "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Morgantown, WV</td>\n",
              "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5aedd60-a4a5-453e-9fe8-63a7e2163593')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5aedd60-a4a5-453e-9fe8-63a7e2163593 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5aedd60-a4a5-453e-9fe8-63a7e2163593');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b054bc9e-cb0d-4a89-9cee-b2c77bbde6e3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b054bc9e-cb0d-4a89-9cee-b2c77bbde6e3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b054bc9e-cb0d-4a89-9cee-b2c77bbde6e3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 11571,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3282.38061473681,\n        \"min\": 0.0,\n        \"max\": 11369.0,\n        \"num_unique_values\": 11370,\n        \"samples\": [\n          3495.0,\n          5461.0,\n          9794.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 219,\n        \"samples\": [\n          \"panicking\",\n          \"evacuate\",\n          \"wreck\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4504,\n        \"samples\": [\n          \"mercado\",\n          \"pittsburgh\",\n          \"Swimming places\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11423,\n        \"samples\": [\n          \"Dying for salvation with dedication No Capitulation, annihilation Papal commendation, reincarnation Heaven is your\\u2026 https://t.co/h65K0KK0Vm\",\n          \"10 years ago, Haiti was devastated by a tragic and catastrophic earthquake. Let\\u2019s take a time today to remember and pra\\u2026\",\n          \"&gt; Get new bicycle saddle &gt; Manual entirely in Chinese &gt; I've got engineering qualifications I'm sure I can figure o\\u2026 https://t.co/mL94RxUiyx\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = data['target'].value_counts()\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_RXiWici4w3",
        "outputId": "373abfcc-db48-44d6-a12c-af1069e1725b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target\n",
            "0    9256\n",
            "1    2315\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data cleaning and the conversion into tokens"
      ],
      "metadata": {
        "id": "_WGqiatPke8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@w+|\\#','', text)  # Remove mentions and hashtags\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove punctuation and numbers\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
        "test_data['cleaned_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Drop the columns if they exist, handling potential KeyError\n",
        "columns_to_drop = ['id', 'location', 'keyword']\n",
        "for col in columns_to_drop:\n",
        "    if col in data.columns:\n",
        "        data.drop(col, axis=1, inplace=True)\n",
        "\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkrIAHl6iDde",
        "outputId": "02bbc40e-365b-4b54-ee8e-794e98bd3eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'target', 'cleaned_text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the word embeddings for the tokens and saving the embedding techniques models in the pickle files.\n"
      ],
      "metadata": {
        "id": "f82vdV8nkrkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec embeddings\n",
        "sentences = data['cleaned_text'].tolist() + test_data['cleaned_text'].tolist()\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "def get_word2vec_embeddings(tokens):\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if valid_tokens:\n",
        "        embedding = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
        "    else:\n",
        "        embedding = np.zeros(100)\n",
        "    return embedding\n",
        "\n",
        "data['embeddings'] = data['cleaned_text'].apply(get_word2vec_embeddings)\n",
        "test_data['embeddings'] = test_data['cleaned_text'].apply(get_word2vec_embeddings)\n",
        "\n",
        "X = np.vstack(data['embeddings'].values)\n",
        "  # Assuming 'target' is the column name for labels\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['target'])\n",
        "\n",
        "word2vec_model_file = 'word2vec_model.pkl'\n",
        "word2vec_model.save(word2vec_model_file)\n",
        "\n",
        "label_encoder_file = 'label_encoder.pkl'\n",
        "joblib.dump(label_encoder, label_encoder_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPjDl6_LiLZl",
        "outputId": "154bb324-0555-4f55-d1f0-1c457c1eda8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model on Random Forest, SVM and Logistic Regression algorithms."
      ],
      "metadata": {
        "id": "Qr4kCojtlPD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Assuming the data is preprocessed and embeddings are generated as shown earlier\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'SVM': SVC(kernel='linear', random_state=42)\n",
        "}\n",
        "\n",
        "accuracies = {}\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    accuracies[model_name] = accuracy\n",
        "\n",
        "\n",
        "    print(f\"{model_name} Classification Report (Validation):\")\n",
        "    print(classification_report(y_val, y_val_pred))\n",
        "    print(f\"{model_name} Accuracy (Validation): {accuracy_score(y_val, y_val_pred)}\\n\")\n",
        "\n",
        " # Find the model with the highest accuracy\n",
        "best_model_name = max(accuracies, key=accuracies.get)\n",
        "best_accuracy = accuracies[best_model_name]\n",
        "\n",
        "print(f\"The model with the highest accuracy is: {best_model_name} with an accuracy of {best_accuracy}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7yHgwz6tXZN",
        "outputId": "a40dfa2c-22c7-446d-96d4-c4d167546844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92      1866\n",
            "           1       0.75      0.51      0.61       449\n",
            "\n",
            "    accuracy                           0.87      2315\n",
            "   macro avg       0.82      0.73      0.76      2315\n",
            "weighted avg       0.86      0.87      0.86      2315\n",
            "\n",
            "Random Forest Accuracy (Validation): 0.8721382289416847\n",
            "\n",
            "Logistic Regression Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91      1866\n",
            "           1       0.68      0.41      0.51       449\n",
            "\n",
            "    accuracy                           0.85      2315\n",
            "   macro avg       0.78      0.68      0.71      2315\n",
            "weighted avg       0.83      0.85      0.83      2315\n",
            "\n",
            "Logistic Regression Accuracy (Validation): 0.8483801295896328\n",
            "\n",
            "SVM Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91      1866\n",
            "           1       0.76      0.33      0.45       449\n",
            "\n",
            "    accuracy                           0.85      2315\n",
            "   macro avg       0.81      0.65      0.68      2315\n",
            "weighted avg       0.84      0.85      0.82      2315\n",
            "\n",
            "SVM Accuracy (Validation): 0.8488120950323974\n",
            "\n",
            "The model with the highest accuracy is: Random Forest with an accuracy of 0.8721382289416847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning using RandomizedSearchCV for the random forest algorithm."
      ],
      "metadata": {
        "id": "h-ADwO4WlhlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7VoLNFhvHFL",
        "outputId": "31aea630-e1c3-444d-d61c-0d2da4ac01ce"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
            "39 fits failed out of a total of 60.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "39 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1145, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 638, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.86235942 0.86084672        nan        nan        nan\n",
            "        nan        nan 0.86138711        nan 0.85771388        nan\n",
            "        nan 0.85868622        nan 0.86106272        nan        nan\n",
            "        nan 0.857606  ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:\n",
            "{'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n",
            "Classification Report after Tuning (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92      1866\n",
            "           1       0.75      0.53      0.62       449\n",
            "\n",
            "    accuracy                           0.87      2315\n",
            "   macro avg       0.82      0.74      0.77      2315\n",
            "weighted avg       0.87      0.87      0.87      2315\n",
            "\n",
            "Accuracy after Tuning (Validation): 0.873866090712743\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_model_tuned.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define a smaller parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "# Initialize the random forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV with fewer iterations\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Perform random search\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\")\n",
        "print(rf_random.best_params_)\n",
        "\n",
        "# Use the best estimator to predict on validation set\n",
        "best_rf = rf_random.best_estimator_\n",
        "y_val_pred_tuned = best_rf.predict(X_val)\n",
        "\n",
        "# Print classification metrics after tuning\n",
        "print(\"Classification Report after Tuning (Validation):\")\n",
        "print(classification_report(y_val, y_val_pred_tuned))\n",
        "print(f\"Accuracy after Tuning (Validation): {accuracy_score(y_val, y_val_pred_tuned)}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model_file = 'best_model_tuned.pkl'\n",
        "joblib.dump(best_rf, best_model_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions and the test data and saving the predictions in predictions.csv"
      ],
      "metadata": {
        "id": "dmPRGkvkmJXu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS46mVMRmb2G",
        "outputId": "cf77cd6f-39d7-404d-c0e2-3d4b91875c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the trained models and other necessary objects\n",
        "best_model_file = 'best_model_tuned.pkl'\n",
        "word2vec_model_file = 'word2vec_model.pkl'\n",
        "label_encoder_file = 'label_encoder.pkl'\n",
        "\n",
        "best_rf = joblib.load(best_model_file)\n",
        "word2vec_model = Word2Vec.load(word2vec_model_file)\n",
        "label_encoder = joblib.load(label_encoder_file)\n",
        "\n",
        "\n",
        "# Prepare the data for prediction\n",
        "X_test = np.vstack(test_data['embeddings'].values)\n",
        "\n",
        "# Predict the sentiments\n",
        "y_test_pred = best_rf.predict(X_test)\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Add predictions to the test data\n",
        "test_data['prediction'] = y_test_pred_labels\n",
        "\n",
        "# Save the predictions to a new file\n",
        "predictions_file = 'predictions.csv'\n",
        "test_data[['text', 'prediction']].to_csv(predictions_file, index=False)\n",
        "\n",
        "print(f\"Predictions saved to {predictions_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model with single sentence depicting a disaster"
      ],
      "metadata": {
        "id": "MtgShNY5mZWF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNwGDArSSQJJ",
        "outputId": "9b787e24-b947-41cf-ce2a-94d9eb255270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for the test sentence 'the Tsunami killed thousands.': [1]\n",
            "Prediction for the test sentence 'the thunderstorm hit our neighbour's house': [1]\n",
            "Prediction for the test sentence 'a house flew in the air due to whirlwind this afternoon': [1]\n",
            "Prediction for the test sentence 'Massive fire has broken out near the oak street bridge in richmond of british colombia': [1]\n",
            "Prediction for the test sentence 'Firefighters are working tirelessly to contain a fire at a local business. Support them by avoiding the area.': [0]\n",
            "Prediction for the test sentence 'Severe thunderstorm warning in effect. Stay indoors and avoid travel if possible.Recovery and rebuilding after the tsunami will require a global effort. Let's stand together.': [1]\n",
            "Prediction for the test sentence 'Tornado spotted! Take cover immediately and follow emergency instructions.': [1]\n",
            "Prediction for the test sentence 'Hurricane making landfall with strong winds and heavy rain. Evacuate if advised and stay indoors.': [1]\n",
            "Prediction for the test sentence 'A strong earthquake has just struck. Check on your loved ones and follow safety protocols.': [1]\n",
            "Prediction for the test sentence 'Smoky skies and blazing fires. Our thoughts are with everyone in the path of the wildfires.': [1]\n",
            "Prediction for the test sentence 'Inundated streets and rising waters. Thoughts are with everyone affected by the floods.': [1]\n",
            "Prediction for the test sentence 'Downed trees and power lines reported due to the storm. Stay away from any fallen wires.': [1]\n"
          ]
        }
      ],
      "source": [
        "# Test the model with a single sentence\n",
        "def test_single_sentence(sentence):\n",
        "    tokens = preprocess_text(sentence)\n",
        "    embedding = get_word2vec_embeddings(tokens)\n",
        "    embedding = embedding.reshape(1, -1)\n",
        "    prediction = best_rf.predict(embedding)\n",
        "    return prediction\n",
        "\n",
        "# Example test with the provided test data\n",
        "test_sentences = [\n",
        "\n",
        "    \"the Tsunami killed thousands.\",\n",
        "    \"the thunderstorm hit our neighbour's house\",\n",
        "    \"a house flew in the air due to whirlwind this afternoon\",\n",
        "    \"Massive fire has broken out near the oak street bridge in richmond of british colombia\",\n",
        "    \"Firefighters are working tirelessly to contain a fire at a local business. Support them by avoiding the area.\",\n",
        "    \"Severe thunderstorm warning in effect. Stay indoors and avoid travel if possible.\"\n",
        "    \"Recovery and rebuilding after the tsunami will require a global effort. Let's stand together.\",\n",
        "    \"Tornado spotted! Take cover immediately and follow emergency instructions.\",\n",
        "    \"Hurricane making landfall with strong winds and heavy rain. Evacuate if advised and stay indoors.\",\n",
        "    \"A strong earthquake has just struck. Check on your loved ones and follow safety protocols.\",\n",
        "    \"Smoky skies and blazing fires. Our thoughts are with everyone in the path of the wildfires.\",\n",
        "    \"Inundated streets and rising waters. Thoughts are with everyone affected by the floods.\",\n",
        "    \"Downed trees and power lines reported due to the storm. Stay away from any fallen wires.\",\n",
        "\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    prediction = test_single_sentence(sentence)\n",
        "    print(f\"Prediction for the test sentence '{sentence}': {prediction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model with single sentence which are not disaster related"
      ],
      "metadata": {
        "id": "ACOxscgRmo4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6yzuFsjr-x",
        "outputId": "57a06f3e-f58c-43b7-9532-ba742b8c6383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for the test sentence 'Since it's yoga day, it's the perfect image to signify that our infrastructure is stretching itself as far towardsthe skies as possible ': [0]\n",
            "Prediction for the test sentence 'The first train to cross the world's highest railway bridge-the Chenab bridge in india': [0]\n",
            "Prediction for the test sentence 'Just had the best ice cream sundae ever!': [0]\n",
            "Prediction for the test sentence 'Spending the afternoon at the park with my dog.': [0]\n",
            "Prediction for the test sentence 'Started a new series on Netflix and I'm hooked.': [0]\n",
            "Prediction for the test sentence 'Had a productive day at work today.': [0]\n",
            "Prediction for the test sentence 'Went for a morning run and feel great!': [0]\n",
            "Prediction for the test sentence 'Cooking a new recipe for dinner tonight.': [0]\n",
            "Prediction for the test sentence 'Just booked tickets for a vacation next month!': [0]\n",
            "Prediction for the test sentence 'Enjoying a peaceful evening with a good book.': [0]\n",
            "Prediction for the test sentence 'Went shopping today and found some great deals.': [0]\n",
            "Prediction for the test sentence 'Attending a friend’s wedding this weekend.': [0]\n",
            "Prediction for the test sentence 'Spent the day at the museum, so much fun!': [0]\n",
            "Prediction for the test sentence 'Just got a promotion at work, feeling accomplished!': [0]\n",
            "Prediction for the test sentence 'Decorating the house for the holidays.': [0]\n",
            "Prediction for the test sentence 'Visited a new coffee shop and loved it.': [0]\n",
            "Prediction for the test sentence 'Had a relaxing spa day, feeling refreshed.': [0]\n",
            "Prediction for the test sentence 'Took a scenic drive through the countryside.': [0]\n",
            "Prediction for the test sentence 'Just finished a challenging puzzle, feeling proud!': [0]\n",
            "Prediction for the test sentence 'Had a game night with friends, it was a blast!': [0]\n",
            "Prediction for the test sentence 'Trying out yoga for the first time today.': [0]\n",
            "Prediction for the test sentence 'Enjoying some homemade cookies with a cup of tea.': [0]\n",
            "Prediction for the test sentence 'Had a fun day at the amusement park.': [0]\n",
            "Prediction for the test sentence 'Learning a new language, it's so interesting!': [0]\n",
            "Prediction for the test sentence 'Went to a farmer's market and bought fresh produce.': [0]\n",
            "Prediction for the test sentence 'Spending quality time with loved ones this weekend.': [0]\n",
            "Prediction for the test sentence 'Attended a fascinating lecture on astronomy.': [0]\n",
            "Prediction for the test sentence 'Just planted a new garden in the backyard.': [0]\n",
            "Prediction for the test sentence 'Exploring new hiking trails this summer.': [0]\n",
            "Prediction for the test sentence 'Enjoying a lazy Sunday afternoon at home.': [0]\n",
            "Prediction for the test sentence 'Volunteering at the local animal shelter today.': [0]\n",
            "Prediction for the test sentence 'Binge-watching my favorite TV series.': [0]\n",
            "Prediction for the test sentence 'Just got a new haircut, feeling great!': [0]\n",
            "Prediction for the test sentence 'Had an amazing dinner date last night.': [0]\n",
            "Prediction for the test sentence 'Trying out new baking recipes in the kitchen.': [0]\n",
            "Prediction for the test sentence 'Went fishing for the first time and caught a big one!': [0]\n",
            "Prediction for the test sentence 'Taking up painting as a new hobby.': [0]\n",
            "Prediction for the test sentence 'Just finished reading an inspiring book.': [0]\n",
            "Prediction for the test sentence 'Spent the day at the beach, soaking up the sun.': [0]\n",
            "Prediction for the test sentence 'Playing board games with the family tonight.': [0]\n",
            "Prediction for the test sentence 'Excited to attend a music festival next month!': [0]\n"
          ]
        }
      ],
      "source": [
        "# Test the model with a single sentence\n",
        "def test_single_sentence(sentence):\n",
        "    tokens = preprocess_text(sentence)\n",
        "    embedding = get_word2vec_embeddings(tokens)\n",
        "    embedding = embedding.reshape(1, -1)\n",
        "    prediction = best_rf.predict(embedding)\n",
        "    return prediction\n",
        "\n",
        "# Example test with the provided test data\n",
        "test_sentences = [\n",
        "    \"Since it's yoga day, it's the perfect image to signify that our infrastructure is stretching itself as far towardsthe skies as possible \",\n",
        "\n",
        "    \"The first train to cross the world's highest railway bridge-the Chenab bridge in india\",\n",
        "    \"Just had the best ice cream sundae ever!\",\n",
        "    \"Spending the afternoon at the park with my dog.\",\n",
        "    \"Started a new series on Netflix and I'm hooked.\",\n",
        "    \"Had a productive day at work today.\",\n",
        "    \"Went for a morning run and feel great!\",\n",
        "    \"Cooking a new recipe for dinner tonight.\",\n",
        "    \"Just booked tickets for a vacation next month!\",\n",
        "    \"Enjoying a peaceful evening with a good book.\",\n",
        "    \"Went shopping today and found some great deals.\",\n",
        "    \"Attending a friend’s wedding this weekend.\",\n",
        "    \"Spent the day at the museum, so much fun!\",\n",
        "    \"Just got a promotion at work, feeling accomplished!\",\n",
        "    \"Decorating the house for the holidays.\",\n",
        "    \"Visited a new coffee shop and loved it.\",\n",
        "    \"Had a relaxing spa day, feeling refreshed.\",\n",
        "    \"Took a scenic drive through the countryside.\",\n",
        "    \"Just finished a challenging puzzle, feeling proud!\",\n",
        "    \"Had a game night with friends, it was a blast!\",\n",
        "    \"Trying out yoga for the first time today.\",\n",
        "    \"Enjoying some homemade cookies with a cup of tea.\",\n",
        "    \"Had a fun day at the amusement park.\",\n",
        "    \"Learning a new language, it's so interesting!\",\n",
        "    \"Went to a farmer's market and bought fresh produce.\",\n",
        "    \"Spending quality time with loved ones this weekend.\",\n",
        "    \"Attended a fascinating lecture on astronomy.\",\n",
        "    \"Just planted a new garden in the backyard.\",\n",
        "    \"Exploring new hiking trails this summer.\",\n",
        "    \"Enjoying a lazy Sunday afternoon at home.\",\n",
        "    \"Volunteering at the local animal shelter today.\",\n",
        "    \"Binge-watching my favorite TV series.\",\n",
        "    \"Just got a new haircut, feeling great!\",\n",
        "    \"Had an amazing dinner date last night.\",\n",
        "    \"Trying out new baking recipes in the kitchen.\",\n",
        "    \"Went fishing for the first time and caught a big one!\",\n",
        "    \"Taking up painting as a new hobby.\",\n",
        "    \"Just finished reading an inspiring book.\",\n",
        "    \"Spent the day at the beach, soaking up the sun.\",\n",
        "    \"Playing board games with the family tonight.\",\n",
        "    \"Excited to attend a music festival next month!\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    prediction = test_single_sentence(sentence)\n",
        "    print(f\"Prediction for the test sentence '{sentence}': {prediction}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cBLTjHHYDQ24",
        "outputId": "68af0bdd-dd2d-4b79-80be-5c98d33e163b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.37.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.4.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.37.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.37.0 watchdog-4.0.1\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement base64 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for base64\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install joblib\n",
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "!pip install requests\n",
        "!pip install base64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RwMG0Ma4noVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kXSbMqeaXAd",
        "outputId": "ce688202-90ca-48ae-94f4-5d381e321c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import requests\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Download NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the trained models and other necessary objects\n",
        "model = joblib.load('best_model_tuned.pkl')\n",
        "word2vec_model = Word2Vec.load('word2vec_model.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Load and encode the background image\n",
        "def get_base64_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode()\n",
        "    return encoded_string\n",
        "\n",
        "#bg_image = get_base64_image('earth-planet.webp')  # Replace with actual image path\n",
        "\n",
        "# Custom CSS\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "    <style>\n",
        "\n",
        "\n",
        "    .stButton>button {{\n",
        "        background-color: #071952;\n",
        "        color: white;\n",
        "        padding: 10px 24px;\n",
        "        font-size: 16px;\n",
        "        border: none;\n",
        "        border-radius: 5px;\n",
        "    }}\n",
        "    .stTextInput>div>div>input {{\n",
        "        padding: 5px;\n",
        "        font-size: 16px;\n",
        "    }}\n",
        "    .disaster {{\n",
        "        color: white;\n",
        "        background-color: red;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "    }}\n",
        "    .non-disaster {{\n",
        "        color: white;\n",
        "        background-color: green;\n",
        "        padding: 10px;\n",
        "        border-radius: 5px;\n",
        "    }}\n",
        "\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True\n",
        ")\n",
        "\n",
        "# Preprocess the input text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)  # Remove mentions and hashtags\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove punctuation and numbers\n",
        "    tokens = nltk.word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "def get_word2vec_embeddings(tokens):\n",
        "    valid_tokens = [word for word in tokens if word in word2vec_model.wv]\n",
        "    if valid_tokens:\n",
        "        embedding = np.mean([word2vec_model.wv[word] for word in valid_tokens], axis=0)\n",
        "    else:\n",
        "        embedding = np.zeros(100)\n",
        "    return embedding\n",
        "\n",
        "# Function to fetch tweet text from URL\n",
        "def fetch_tweet_text_from_url(url, bearer_token):\n",
        "    tweet_id = url.split('/')[-1]\n",
        "    tweet_url = f\"https://api.twitter.com/2/tweets/{tweet_id}\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {bearer_token}',\n",
        "    }\n",
        "    response = requests.get(tweet_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        st.error(f\"Error fetching content from URL: {response.status_code}\")\n",
        "        st.write(response.json())  # Debugging step to show the response content\n",
        "        return None\n",
        "    tweet = response.json()\n",
        "    return tweet['data']['text']\n",
        "\n",
        "# Function to fetch disaster tweets\n",
        "\n",
        "\n",
        "def fetch_disaster_tweets(timeframe, bearer_token):\n",
        "    query = \"disaster OR flood OR earthquake OR hurricane OR tornado OR tsunami OR wildfire OR landslide OR eruption OR cyclone OR drought OR blizzard OR storm OR catastrophe OR calamity OR emergency OR plane crash -is:retweet\"\n",
        "\n",
        "    end_time = datetime.utcnow() - timedelta(seconds=10)  # Ensure end_time is at least 10 seconds before current time\n",
        "    start_time = end_time - timedelta(hours=4 if timeframe == \"Last 4 hours\" else 24)\n",
        "    start_time_str = start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "    end_time_str = end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "    search_url = f\"https://api.twitter.com/2/tweets/search/recent?query={query}&start_time={start_time_str}&end_time={end_time_str}&max_results=10\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {bearer_token}',\n",
        "    }\n",
        "\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        st.error(f\"Error fetching tweets: {response.status_code}\")\n",
        "        st.write(response.json())  # Debugging step to show the response content\n",
        "        return []\n",
        "    tweets = response.json().get('data', [])\n",
        "    users = {user['id']: user['location'] for user in response.json().get('includes', {}).get('users', [])}\n",
        "\n",
        "\n",
        "    disaster_tweets = []\n",
        "    for tweet in tweets:\n",
        "        tweet_text = tweet['text']\n",
        "        cleaned_text = preprocess_text(tweet_text)\n",
        "        embedding = get_word2vec_embeddings(cleaned_text).reshape(1, -1)\n",
        "        prediction = model.predict(embedding)\n",
        "        prediction_label = label_encoder.inverse_transform(prediction)[0]\n",
        "        if prediction_label == 1:  # Assuming 1 represents disaster\n",
        "            user_id = tweet['author_id']\n",
        "            user_location = users.get(tweet['author_id'], 'Unknown location')\n",
        "            disaster_tweets.append({'text': tweet_text, 'location': user_location})\n",
        "\n",
        "    return disaster_tweets\n",
        "\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.markdown(\"<div class='main'>\", unsafe_allow_html=True)\n",
        "    st.title(\"Disaster Tweet Classifier\")\n",
        "    st.write(\"Enter a tweet or a URL leading to a tweet to predict if it's a disaster tweet or not.\")\n",
        "    tweet_or_url = st.text_area(\"Enter Tweet Text or URL\")\n",
        "\n",
        "    if st.button(\"Fetch & Classify\"):\n",
        "        if tweet_or_url.startswith(\"http\"):\n",
        "            tweet_text = fetch_tweet_text_from_url(tweet_or_url, 'insert_your_bearer_token_here')  # Replace with your actual bearer token\n",
        "            if tweet_text:\n",
        "                st.write(f\"Tweet: {tweet_text}\")\n",
        "                cleaned_text = preprocess_text(tweet_text)\n",
        "            else:\n",
        "                st.error(\"Failed to fetch content from URL.\")\n",
        "                return\n",
        "        else:\n",
        "            tweet_text = tweet_or_url\n",
        "            cleaned_text = preprocess_text(tweet_or_url)\n",
        "\n",
        "        embedding = get_word2vec_embeddings(cleaned_text).reshape(1, -1)\n",
        "        prediction = model.predict(embedding)\n",
        "        prediction_label = label_encoder.inverse_transform(prediction)[0]\n",
        "\n",
        "        if prediction_label == 1:\n",
        "            st.markdown(f\"<h3 class='disaster'>Prediction: Disaster Tweet</h3>\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f\"<h3 class='non-disaster'>Prediction: Not a Disaster Tweet</h3>\", unsafe_allow_html=True)\n",
        "\n",
        "    st.write(\"## Fetch Recent Disaster Tweets\")\n",
        "    timeframe = st.selectbox(\"Select timeframe\", [\"Last 24 hours\", \"Last 4 hours\"])\n",
        "    if st.button(\"Fetch Disaster Tweets\"):\n",
        "        tweets = fetch_disaster_tweets(timeframe, 'Replace with your actual bearer token')  # Replace with your actual bearer token\n",
        "        if tweets:\n",
        "            for tweet in tweets:\n",
        "                st.write(f\"- {tweet['text']}\")\n",
        "        else:\n",
        "            st.write(\"No recent disaster tweets found.\")\n",
        "\n",
        "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy93J4lLXc5U",
        "outputId": "b3162f02-443f-45ac-dd77-b0eda6983a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://a676-34-139-251-178.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Set up your ngrok authentication token\n",
        "ngrok.set_auth_token('Replace with your actual ngrok auth token')  # Replace with your actual ngrok auth token\n",
        "\n",
        "# Kill any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Run the Streamlit app\n",
        "streamlit_proc = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n",
        "\n",
        "# Wait a few seconds for the app to start\n",
        "time.sleep(20)  # Increase the sleep time if necessary\n",
        "\n",
        "# Create a tunnel to the Streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    streamlit_proc.communicate()\n",
        "except KeyboardInterrupt:\n",
        "    streamlit_proc.terminate()\n",
        "    ngrok.kill()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8k8o3L0LVN2K+6op+ewNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}